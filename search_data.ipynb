{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install joblib nltk requests bs4 ipywidgets xgboost pandas numpy seaborn scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05ff9a3283fd4fbfb6ccecaddbcf5d9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='Link:', placeholder='Enter article link')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93a3f428d15b4279a715bda6e4bcc56b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Fetch Article', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# Create input widget\n",
    "link_input = widgets.Text(\n",
    "    placeholder='Enter article link',\n",
    "    description='Link:',\n",
    ")\n",
    "\n",
    "# Create a button widget\n",
    "fetch_button = widgets.Button(\n",
    "    description='Fetch Article',\n",
    ")\n",
    "\n",
    "# Create a variable to store the link\n",
    "article_link = None\n",
    "\n",
    "# Define a function to save the link to the variable\n",
    "def save_article_link(b):\n",
    "    global article_link\n",
    "    article_link = link_input.value\n",
    "\n",
    "# Attach the function to the button click event\n",
    "fetch_button.on_click(save_article_link)\n",
    "\n",
    "# Display the widgets\n",
    "display(link_input, fetch_button)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Australia: Sydney bans fires - including barbecues - as hot and dry conditions bring wildfire risk\n",
      "Text: A complete ban on fires is declared for the Greater Sydney area and coastal communities to the south, where 20 schools have been closed.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Define a function to fetch and process the article\n",
    "def fetch_and_process_article():\n",
    "    # Check if the article link has been provided\n",
    "    if article_link is None:\n",
    "        print(\"Please enter the article link and click 'Fetch Article' in the previous cell.\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        # Fetch the article\n",
    "        response = requests.get(article_link)\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        \n",
    "        # Find the title\n",
    "        title_element = soup.find(\"h1\")\n",
    "        title = title_element.text if title_element else \"Title not found\"\n",
    "        \n",
    "        # Find the text content\n",
    "        content_element = soup.find(\"p\")\n",
    "        text = content_element.text if content_element else \"Text not found\"\n",
    "        \n",
    "        # Display the results\n",
    "        print(f\"Title: {title}\")\n",
    "        print(f\"Text: {text}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "# Call the function to fetch and process the article\n",
    "fetch_and_process_article()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The article is classified as REAL.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import joblib\n",
    "import xgboost\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Fetch and process the article\n",
    "def fetch_article(article_link):\n",
    "    try:\n",
    "        # Fetch the article\n",
    "        response = requests.get(article_link)\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        # Find the title\n",
    "        title_element = soup.find(\"h1\")\n",
    "        title = title_element.text if title_element else \"Title not found\"\n",
    "\n",
    "        return title\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Preprocess text using NLTK\n",
    "def preprocess_text(text):\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "\n",
    "    # Lemmatize the tokens\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    # Join the tokens back into a single string\n",
    "    preprocessed_text = \" \".join(tokens)\n",
    "\n",
    "    return preprocessed_text\n",
    "\n",
    "# Load the saved vectorizer\n",
    "vectorizer = joblib.load(\"onion_vectorizer.pkl\")\n",
    "\n",
    "# load classifier\n",
    "classifier = joblib.load(\"onion_xgboost_classifier.pkl\")\n",
    "\n",
    "def classify_article(article_link, classifier):\n",
    "    title = fetch_article(article_link)\n",
    "    \n",
    "    if title is not None:\n",
    "        \n",
    "        # Preprocess the article text\n",
    "        preprocessed_article = preprocess_text(title)\n",
    "        \n",
    "        # Transform the input data using the same vectorizer\n",
    "        input_tfidf = vectorizer.transform([preprocessed_article])\n",
    "        \n",
    "        # Predict using the classifier\n",
    "        prediction = classifier.predict(input_tfidf)\n",
    "\n",
    "        # Output the prediction (0 for fake, 1 for real)\n",
    "        if prediction[0] == 0:\n",
    "            return \"The article is classified as FAKE.\"\n",
    "        else:\n",
    "            return \"The article is classified as REAL.\"\n",
    "    else:\n",
    "        return \"Article processing failed.\"\n",
    "\n",
    "result = classify_article(article_link, classifier)\n",
    "print(result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
