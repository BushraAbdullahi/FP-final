{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping /usr/local/lib/python3.11/site-packages/certifi-2023.7.22-py3.11.egg-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /usr/local/lib/python3.11/site-packages/certifi-2023.7.22-py3.11.egg-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: nltk in /usr/local/Cellar/jupyterlab/4.0.6/libexec/lib/python3.11/site-packages (3.8.1)\n",
      "Requirement already satisfied: joblib in /usr/local/Cellar/jupyterlab/4.0.6/libexec/lib/python3.11/site-packages (1.3.2)\n",
      "Requirement already satisfied: requests in /usr/local/Cellar/jupyterlab/4.0.6/libexec/lib/python3.11/site-packages (2.31.0)\n",
      "Requirement already satisfied: bs4 in /usr/local/Cellar/jupyterlab/4.0.6/libexec/lib/python3.11/site-packages (0.0.1)\n",
      "Requirement already satisfied: ipywidgets in /usr/local/Cellar/jupyterlab/4.0.6/libexec/lib/python3.11/site-packages (8.1.1)\n",
      "Requirement already satisfied: xgboost in /usr/local/Cellar/jupyterlab/4.0.6/libexec/lib/python3.11/site-packages (2.0.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/site-packages (1.5.3)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/site-packages (1.24.2)\n",
      "Requirement already satisfied: seaborn in /usr/local/Cellar/jupyterlab/4.0.6/libexec/lib/python3.11/site-packages (0.12.2)\n",
      "Collecting scikit-learn\n",
      "  Obtaining dependency information for scikit-learn from https://files.pythonhosted.org/packages/e8/dd/41bc4dfa519bc1a0617b68496120c472f1a1a5db264132d1530c43f059a8/scikit_learn-1.3.0-cp311-cp311-macosx_10_9_x86_64.whl.metadata\n",
      "  Downloading scikit_learn-1.3.0-cp311-cp311-macosx_10_9_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.11/site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/Cellar/jupyterlab/4.0.6/libexec/lib/python3.11/site-packages (from nltk) (2023.8.8)\n",
      "Requirement already satisfied: tqdm in /usr/local/Cellar/jupyterlab/4.0.6/libexec/lib/python3.11/site-packages (from nltk) (4.66.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/Cellar/jupyterlab/4.0.6/libexec/lib/python3.11/site-packages (from requests) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/Cellar/jupyterlab/4.0.6/libexec/lib/python3.11/site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/Cellar/jupyterlab/4.0.6/libexec/lib/python3.11/site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/opt/python-certifi/lib/python3.11/site-packages (from requests) (2023.7.22)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/Cellar/jupyterlab/4.0.6/libexec/lib/python3.11/site-packages (from bs4) (4.12.2)\n",
      "Requirement already satisfied: comm>=0.1.3 in /usr/local/Cellar/jupyterlab/4.0.6/libexec/lib/python3.11/site-packages (from ipywidgets) (0.1.4)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /usr/local/opt/ipython/libexec/lib/python3.11/site-packages (from ipywidgets) (8.15.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/opt/ipython/libexec/lib/python3.11/site-packages (from ipywidgets) (5.9.0)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.9 in /usr/local/Cellar/jupyterlab/4.0.6/libexec/lib/python3.11/site-packages (from ipywidgets) (4.0.9)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.9 in /usr/local/Cellar/jupyterlab/4.0.6/libexec/lib/python3.11/site-packages (from ipywidgets) (3.0.9)\n",
      "Requirement already satisfied: scipy in /usr/local/Cellar/jupyterlab/4.0.6/libexec/lib/python3.11/site-packages (from xgboost) (1.11.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/Cellar/jupyterlab/4.0.6/libexec/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/site-packages (from pandas) (2022.7.1)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.1 in /usr/local/lib/python3.11/site-packages (from seaborn) (3.7.1)\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn)\n",
      "  Obtaining dependency information for threadpoolctl>=2.0.0 from https://files.pythonhosted.org/packages/81/12/fd4dea011af9d69e1cad05c75f3f7202cdcbeac9b712eea58ca779a72865/threadpoolctl-3.2.0-py3-none-any.whl.metadata\n",
      "  Downloading threadpoolctl-3.2.0-py3-none-any.whl.metadata (10.0 kB)\n",
      "Requirement already satisfied: backcall in /usr/local/opt/ipython/libexec/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: decorator in /usr/local/opt/ipython/libexec/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /usr/local/opt/ipython/libexec/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.0)\n",
      "Requirement already satisfied: matplotlib-inline in /usr/local/opt/ipython/libexec/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: pickleshare in /usr/local/opt/ipython/libexec/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /usr/local/opt/ipython/libexec/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.39)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /usr/local/opt/pygments/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (2.16.1)\n",
      "Requirement already satisfied: stack-data in /usr/local/opt/ipython/libexec/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.2)\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/local/opt/ipython/libexec/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: appnope in /usr/local/opt/ipython/libexec/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (4.39.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/opt/python-packaging/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (9.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (3.0.9)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/opt/six/lib/python3.11/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/Cellar/jupyterlab/4.0.6/libexec/lib/python3.11/site-packages (from beautifulsoup4->bs4) (2.5)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/opt/ipython/libexec/lib/python3.11/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/opt/ipython/libexec/lib/python3.11/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/opt/ipython/libexec/lib/python3.11/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=6.1.0->ipywidgets) (0.2.6)\n",
      "Requirement already satisfied: executing>=1.2.0 in /usr/local/opt/ipython/libexec/lib/python3.11/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (1.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /usr/local/opt/ipython/libexec/lib/python3.11/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.2.1)\n",
      "Requirement already satisfied: pure-eval in /usr/local/opt/ipython/libexec/lib/python3.11/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Using cached scikit_learn-1.3.0-cp311-cp311-macosx_10_9_x86_64.whl (10.1 MB)\n",
      "Using cached threadpoolctl-3.2.0-py3-none-any.whl (15 kB)\n",
      "\u001b[33mWARNING: Skipping /usr/local/lib/python3.11/site-packages/certifi-2023.7.22-py3.11.egg-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: threadpoolctl, scikit-learn\n",
      "Successfully installed scikit-learn-1.3.0 threadpoolctl-3.2.0\n",
      "\u001b[33mWARNING: Skipping /usr/local/lib/python3.11/site-packages/certifi-2023.7.22-py3.11.egg-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /usr/local/lib/python3.11/site-packages/certifi-2023.7.22-py3.11.egg-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /usr/local/lib/python3.11/site-packages/certifi-2023.7.22-py3.11.egg-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk joblib requests bs4 ipywidgets xgboost pandas numpy seaborn scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title      0\n",
       "text       0\n",
       "subject    0\n",
       "date       0\n",
       "label      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "csv_file_path = \"preprocessed_data_2.csv\"\n",
    "\n",
    "preprocessed_df = pd.read_csv(csv_file_path)\n",
    "\n",
    "preprocessed_df[\"text\"].fillna(\"\", inplace=True)\n",
    "preprocessed_df.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9974320065367106\n",
      "Precision: 0.9974293059125964\n",
      "Recall: 0.9974293059125964\n",
      "F1-score: 0.9974293059125964\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Split the data into train and test sets\n",
    "train_df, test_df = train_test_split(preprocessed_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit the vectorizer on the training text\n",
    "vectorizer.fit(train_df[\"text\"])\n",
    "\n",
    "# Transform the training and test text into TF-IDF feature vectors\n",
    "train_tfidf = vectorizer.transform(train_df[\"text\"])\n",
    "test_tfidf = vectorizer.transform(test_df[\"text\"])\n",
    "\n",
    "# Get the corresponding labels\n",
    "train_labels = train_df[\"label\"]\n",
    "test_labels = test_df[\"label\"]\n",
    "\n",
    "# Train an XGBoost classifier\n",
    "classifier = xgb.XGBClassifier()  # Initialize XGBoost classifier\n",
    "classifier.fit(train_tfidf, train_labels)\n",
    "\n",
    "# Predict on the test set\n",
    "predictions = classifier.predict(test_tfidf)\n",
    "\n",
    "# Calculate the evaluation metrics\n",
    "accuracy = accuracy_score(test_labels, predictions)\n",
    "precision = precision_score(test_labels, predictions)\n",
    "recall = recall_score(test_labels, predictions)\n",
    "f1 = f1_score(test_labels, predictions)\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "conf_matrix = confusion_matrix(test_labels, predictions)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1-score: {f1}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffd8fab42e4f453dbb054b7c00a8522e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='Link:', placeholder='Enter article link')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ad63b1e9955401c8a1c86724373ffa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Fetch Article', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# Create input widget\n",
    "link_input = widgets.Text(\n",
    "    placeholder='Enter article link',\n",
    "    description='Link:',\n",
    ")\n",
    "\n",
    "# Create a button widget\n",
    "fetch_button = widgets.Button(\n",
    "    description='Fetch Article',\n",
    ")\n",
    "\n",
    "# Create a variable to store the link\n",
    "article_link = None\n",
    "\n",
    "# Define a function to save the link to the variable\n",
    "def save_article_link(b):\n",
    "    global article_link\n",
    "    article_link = link_input.value\n",
    "\n",
    "# Attach the function to the button click event\n",
    "fetch_button.on_click(save_article_link)\n",
    "\n",
    "# Display the widgets\n",
    "display(link_input, fetch_button)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Every American Exchanged For Iranian Population In First Successful Citizen Swap Deal\n",
      "Text: WASHINGTON—In the first full-scale successful citizen swap of its kind, the White House confirmed Monday that it had exchanged every person in America for the entire population of Iran. “I’m proud to announce that we have transferred 335 million Americans to Iran,” said Biden, who called the swap a “monumental” first step toward thawing diplomatic relations between the two nations, as millions of confused former residents of Washington, D.C. woke up to find themselves in the arid climate of Tehran, and millions of baffled former residents of Tehran woke up in Washington, D.C., rubbing their eyes and blinking as they stared at the completely unfamiliar territory outside the bedroom windows of homes that were not theirs. “I have heard criticism from Republican leaders, but those guys don’t live here anymore. This is a historic day for relations between America and Iran. Or should I say Iran and America?” At press time, President Biden and Iranian President Ebrahim Raisi were arguing about whether the two of them were supposed to stay put or swap places as well. \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Define a function to fetch and process the article\n",
    "def fetch_and_process_article():\n",
    "    # Check if the article link has been provided\n",
    "    if article_link is None:\n",
    "        print(\"Please enter the article link and click 'Fetch Article' in the previous cell.\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        # Fetch the article\n",
    "        response = requests.get(article_link)\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        \n",
    "        # Find the title\n",
    "        title_element = soup.find(\"h1\")\n",
    "        title = title_element.text if title_element else \"Title not found\"\n",
    "        \n",
    "        # Find the text content\n",
    "        content_element = soup.find(\"p\")\n",
    "        text = content_element.text if content_element else \"Text not found\"\n",
    "        \n",
    "        # Display the results\n",
    "        print(f\"Title: {title}\")\n",
    "        print(f\"Text: {text}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "# Call the function to fetch and process the article\n",
    "fetch_and_process_article()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The article is classified as REAL.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import joblib\n",
    "import xgboost\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Fetch and process the article\n",
    "def fetch_and_process_article(article_link):\n",
    "    try:\n",
    "        # Fetch the article\n",
    "        response = requests.get(article_link)\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        # Find the title\n",
    "        title_element = soup.find(\"h1\")\n",
    "        title = title_element.text if title_element else \"Title not found\"\n",
    "\n",
    "        # Find the text content\n",
    "        content_element = soup.find(\"p\")\n",
    "        text = content_element.text if content_element else \"Text not found\"\n",
    "\n",
    "        # Preprocess the title and text\n",
    "        title = preprocess_text(title)\n",
    "        text = preprocess_text(text)\n",
    "\n",
    "        return title, text\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Preprocess text using NLTK\n",
    "def preprocess_text(text):\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "\n",
    "    # Lemmatize the tokens\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    # Join the tokens back into a single string\n",
    "    preprocessed_text = \" \".join(tokens)\n",
    "\n",
    "    return preprocessed_text\n",
    "\n",
    "# Classify the article as real or fake\n",
    "def classify_article(article_link, classifier):\n",
    "    title, text = fetch_and_process_article(article_link)\n",
    "    \n",
    "    if title is not None and text is not None:\n",
    "        \n",
    "        # Preprocess the article text\n",
    "        preprocessed_article = preprocess_text(text)\n",
    "        \n",
    "        # Transform the input data using the same vectorizer\n",
    "        input_tfidf = vectorizer.transform([preprocessed_article])\n",
    "        \n",
    "        # Predict using the classifier\n",
    "        prediction = classifier.predict(input_tfidf)\n",
    "\n",
    "        # Output the prediction (0 for fake, 1 for real, adjust as needed)\n",
    "        if prediction[0] == 0:\n",
    "            return \"The article is classified as FAKE.\"\n",
    "        else:\n",
    "            return \"The article is classified as REAL.\"\n",
    "    else:\n",
    "        return \"Article processing failed.\"\n",
    "\n",
    "result = classify_article(article_link, classifier)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
